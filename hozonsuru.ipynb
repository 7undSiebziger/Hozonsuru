{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization and Pruning Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "\n",
    "# set global seeds for reproducibility\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Setting parameters for plotting\n",
    "plt.rcParams['figure.figsize'] = (15.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is available\n",
    "print(\"GPU is\", \"AVAILABLE\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather Dataset\n",
    "dataset_path = \"dataset\"\n",
    "\n",
    "# Function to load dataset\n",
    "class_labels = [label for label in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, label))]\n",
    "class_to_index = {label: index for index, label in enumerate(class_labels)}\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dataset_path):\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for image_file in os.listdir(label_path):\n",
    "                image_path = os.path.join(label_path, image_file)\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is None:\n",
    "                    continue  # Skip the image if it cannot be loaded\n",
    "                image = cv2.resize(image, (96, 96))\n",
    "                images.append(image)\n",
    "                labels.append(class_to_index[label])\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "all_images, all_labels = load_dataset(dataset_path)\n",
    "\n",
    "\n",
    "# Check if dataset is loaded correctly\n",
    "if len(all_images) == 0:\n",
    "    raise ValueError(\"No images loaded. Please check the dataset path and ensure it contains images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "all_images = all_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "We can visualize the data by plotting 5 samples of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "\n",
    "#classes = ['cloudy', 'foggy', 'rainy', 'shine', 'sunrise']\n",
    "classes = [i for i in range(len(class_labels))]\n",
    "print(classes)\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 2\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(train_labels == cls)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(train_images[idx])\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "fp_model = load_model('model.h5', compile=False)\n",
    "\n",
    "# # Use an optimizer without weight_decay\n",
    "# optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# # Compile the model with the correct optimizer\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "fp_model.compile(optimizer='adam',\n",
    "                # the loss function is the sparse categorical cross-entropy\n",
    "                # loss. It is used when there are two or more label classes. \n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the model summary\n",
    "fp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More training can be done here\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "history = fp_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=1,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model \n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "fp_model.save('models/base_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on the test set\n",
    "fp_test_loss, fp_test_acc = fp_model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('Test accuracy:', fp_test_acc)\n",
    "print('Test loss:', fp_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix for the quantized model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "\"\"\"\n",
    "function: plot_confusion_matrix\n",
    "    - input: cm, classes, normalize, title, cmap\n",
    "    - output: none\n",
    "    - description: plots the confusion matrix\n",
    "\"\"\"\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                            normalize=False,\n",
    "                            title='Confusion matrix',\n",
    "                            cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, np.argmax(fp_model.predict(test_images), axis=-1))\n",
    "plot_confusion_matrix(cm, classes, title='Confusion matrix, without normalization', cmap=plt.cm.Spectral.reversed())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to a tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to TFLite without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(fp_model)\n",
    "fp_tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"models/model_f32.tflite\", \"wb\").write(fp_tflite_model)\n",
    "\n",
    "# Show the model size for the non-quantized HDF5 model\n",
    "fp_h5_in_kb = os.path.getsize('models/base_model.h5') / 1024\n",
    "print(\"HDF5 Model size without quantization: %d KB\" % fp_h5_in_kb)\n",
    "\n",
    "# Show the model size for the non-quantized TFLite model\n",
    "fp_tflite_in_kb = os.path.getsize('models/model_f32.tflite') / 1024\n",
    "print(\"TFLite Model size without quantization: %d KB\" % fp_tflite_in_kb)\n",
    "\n",
    "# Determine the reduction in model size\n",
    "print(\"\\nReduction in file size by a factor of %f\" % (fp_h5_in_kb / fp_tflite_in_kb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize with dynamic range optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to TFLite with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(fp_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "dynR_quant_tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"models/model_quant8_dynR.tflite\", \"wb\").write(dynR_quant_tflite_model)\n",
    "\n",
    "print(\"Model was saved at location: %s\" % os.path.abspath('models/fmodel_quant8_dynR.tflite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=dynR_quant_tflite_model)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the model to 8-bit integer precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(fp_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_int8 = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_int8)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model to disk\n",
    "open(\"models/full_quant.tflite\", \"wb\").write(tflite_model_quant_int8)\n",
    "\n",
    "print(\"Model was saved at location: %s\" % os.path.abspath('models/full_quant.tflite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the model size for the 8-bit quantized TFLite model\n",
    "tflite_quant_in_kb = os.path.getsize('models/full_quant.tflite') / 1024\n",
    "print(\"TFLite Model size with 8-bit quantization: %d KB\" % tflite_quant_in_kb)\n",
    "\n",
    "print(\"TFLite Model size without quantization: %d KB\" % fp_tflite_in_kb)\n",
    "\n",
    "# Determine the reduction in model size\n",
    "print(\"\\nReduction in model size by a factor of %f\" % (fp_tflite_in_kb / tflite_quant_in_kb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation of TF Lite models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  global test_images\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = test_images[test_image_index]\n",
    "    test_label = test_labels[test_image_index]\n",
    "\n",
    "    if (test_image_index % 1000 == 0):\n",
    "      print(\"Evaluated on %d images.\" % test_image_index)\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return predictions\n",
    "\n",
    "\n",
    "# Helper function to evaluate a TFLite model on all images\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "  global test_images\n",
    "  global test_labels\n",
    "\n",
    "  test_image_indices = range(test_images.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "\n",
    "  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_quant_int8_file = pathlib.Path('models/full_quant.tflite')\n",
    "tflite_model_quant_int8_model_type = \"Full Post-Quantized INT8\"\n",
    "\n",
    "evaluate_model(tflite_model_quant_int8_file, tflite_model_quant_int8_model_type)\n",
    "print(\"Full-precision model accuracy is %.4f%% (Number of test samples=%d)\" % (fp_test_acc * 100, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot also the confusion matrix of the quantized model\n",
    "tflite_model_quant_int8_pred = run_tflite_model(tflite_model_quant_int8_file, range(test_images.shape[0]))\n",
    "cm = confusion_matrix(test_labels, tflite_model_quant_int8_pred);\n",
    "plot_confusion_matrix(cm, classes, title='Confusion matrix, without normalization of the quantized model', cmap=plt.cm.Spectral.reversed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy of the quantized model\n",
    "from sklearn.metrics import accuracy_score\n",
    "full_int8_accuracy = accuracy_score(test_labels, tflite_model_quant_int8_pred)\n",
    "print(\"Full-precision model accuracy is %.4f%% (Number of test samples=%d)\" % (fp_test_acc * 100, len(test_images)))\n",
    "print(\"Quantized model accuracy is %.4f%% (Number of test samples=%d)\" % (full_int8_accuracy * 100, len(test_images)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the model to a C header file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nstatic const unsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'static const unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'weather_model'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model_quant_int8, c_model_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving test data used for inference on the MCU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the test data as numpy arrays\n",
    "int_test_images = (test_images*255).astype(np.uint8)\n",
    "\n",
    "np.save('x_test.npy', int_test_images.astype(np.uint8))\n",
    "np.save('y_test.npy', test_labels.astype(np.uint8))\n",
    "# plot the first 5 images in the test set with their labels\n",
    "# map class labels to names\n",
    "class_names = ['cloudy', 'foggy', 'rainy', 'shine', 'sunrise']\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(int_test_images.astype(np.uint8)[i], cmap='gray')\n",
    "    plt.title('Label: %s' % class_names[test_labels[i]])\n",
    "\n",
    "# print the location of the files\n",
    "print('Test image data location: ', os.path.abspath('x_test.npy'))\n",
    "print('Test labels location: ', os.path.abspath('y_test.npy'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization-aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Convert the model to a quantization aware model\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(fp_model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "quant_aware_model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "quant_loss, quant_acc = quant_aware_model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Quantization aware training loss: ', quant_loss)\n",
    "print('Quantization aware training accuracy: ', quant_acc)\n",
    "print('Full-precision training accuracy: ', fp_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_int8_qat = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant_int8_qat)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "# Save the quantized model to disk\n",
    "open(\"models/fmnist_qat_int8.tflite\", \"wb\").write(tflite_model_quant_int8_qat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'qat8weather'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model_quant_int8_qat, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this might take a few minutes (~ 1- 2 minutes)\n",
    "# if it takes longer than that, I suggest to \n",
    "# restart the runtime and try again\n",
    "# if the issue still persists, restart your computer\n",
    "tflite_model_quant_int8_qat_file = pathlib.Path('models/qat_int8.tflite')\n",
    "tflite_model_quant_int8_qat_type = \"Full QAT INT8\"\n",
    "\n",
    "evaluate_model(tflite_model_quant_int8_qat_file, tflite_model_quant_int8_qat_type)\n",
    "print(\"Full-precision model accuracy is %.4f%% (Number of test samples=%d)\" % (fp_test_acc * 100, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "tflite_model_quant_int8_qat_file = \"models/qat_int8.tflite\"\n",
    "tflite_model_quant_int8_qat_model_type = \"Quantized aware training model\"\n",
    "tflite_model_quant_int8_qat_pred = run_tflite_model(tflite_model_quant_int8_qat_file, range(test_images.shape[0]))\n",
    "cm = confusion_matrix(test_labels, tflite_model_quant_int8_qat_pred);\n",
    "plot_confusion_matrix(cm, classes, title='Confusion matrix, without normalization of the quantized model', cmap=plt.cm.Spectral.reversed())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy of the model\n",
    "full_qat_int8_accuracy = accuracy_score(test_labels, tflite_model_quant_int8_qat_pred)\n",
    "print('Full QAT INT8 accuracy is %.4f%% (Number of test samples=%d)' % (full_qat_int8_accuracy * 100, len(test_images)))\n",
    "print('Full-precision model accuracy is %.4f%% (Number of test samples=%d)' % (fp_test_acc * 100, len(test_images)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning with constant sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "\n",
    "# Strucutred pruning with constant sparsity\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=2000, frequency=100),\n",
    "        'block_size': (1, 1),\n",
    "}\n",
    "\n",
    "# Create a pruning model\n",
    "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(fp_model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "pruned_model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the pruned model\n",
    "pruned_model.fit(\n",
    "                    train_images,\n",
    "                    train_labels,\n",
    "                    epochs=2,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_loss, pruned_acc = pruned_model.evaluate(train_images, train_labels, verbose=0)\n",
    "print('Pruned model loss: ', pruned_loss)\n",
    "print('Pruned model accuracy: ', pruned_acc)\n",
    "print('Full-precision model accuracy: ', fp_test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to TF Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "pruned_keras_file = 'models/pruned_model.h5'\n",
    "tf.keras.models.save_model(pruned_model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', os.path.abspath(pruned_keras_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model_for_export)\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "pruned_tflite_file = 'models/pruned_model.tflite'\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "    f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', os.path.abspath(pruned_tflite_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'prunedweather'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(pruned_tflite_model, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to actually compress the models via gzip and measure the zipped size.\n",
    "import tempfile\n",
    "\n",
    "def get_gzipped_model_size(file):\n",
    "    # It returns the size of the gzipped model in bytes.\n",
    "    import os\n",
    "    import zipfile\n",
    "    \n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "    \n",
    "    return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the pruned model: ', get_gzipped_model_size(pruned_tflite_file))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('models/model_f32.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('models/model_f32.tflite') / get_gzipped_model_size(pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_tflite_file = \"models/pruned_model.tflite\"\n",
    "pruned_tflite_model_type = \"Pruned model\"\n",
    "evaluate_model(pruned_tflite_file, pruned_tflite_model_type)\n",
    "print(\"Full-precision model accuracy is %.4f%% (Number of test samples=%d)\" % (fp_test_acc * 100, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_tflite_file = \"models/pruned_model.tflite\"\n",
    "pruned_tflite_model_type = \"Pruned model\"\n",
    "pruned_tflite_pred = run_tflite_model(pruned_tflite_file, range(test_images.shape[0]))\n",
    "cm = confusion_matrix(test_labels, pruned_tflite_pred);\n",
    "plot_confusion_matrix(cm, classes, title='Confusion matrix, without normalization of the pruned model', cmap=plt.cm.Spectral.reversed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy of the model\n",
    "pruned_accuracy = accuracy_score(test_labels, pruned_tflite_pred)\n",
    "print('Pruned model accuracy is %.4f%% (Number of test samples=%d)' % (pruned_accuracy * 100, len(test_images)))\n",
    "print('Full-precision model accuracy is %.4f%% (Number of test samples=%d)' % (fp_test_acc * 100, len(test_images)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Unstructured pruning with constant sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstrucutred pruning with constant sparsity\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=2000, frequency=100),\n",
    "}\n",
    "\n",
    "# Create a pruning model\n",
    "pruned_model_unstructured = tfmot.sparsity.keras.prune_low_magnitude(fp_model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "pruned_model_unstructured.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_model_unstructured.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the pruned model\n",
    "pruned_model_unstructured.fit(\n",
    "                    train_images,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_loss_unstructured, pruned_acc_unstructured = pruned_model_unstructured.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Pruned model loss: ', pruned_loss_unstructured)\n",
    "print('Pruned model accuracy: ', pruned_acc_unstructured)\n",
    "print('Full-precision model accuracy: ', fp_test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to TF Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to TF Lite\n",
    "pruned_model_unstructured_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model_unstructured)\n",
    "\n",
    "pruned_keras_file_unstructured = 'models/pruned_model_unstructured.h5'\n",
    "tf.keras.models.save_model(pruned_model_unstructured_for_export, pruned_keras_file_unstructured, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', os.path.abspath(pruned_keras_file_unstructured))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model_unstructured_for_export)\n",
    "pruned_tflite_model_unstructured = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "pruned_tflite_file_unstructured = 'models/pruned_model_unstructured.tflite'\n",
    "\n",
    "with open(pruned_tflite_file_unstructured, 'wb') as f:\n",
    "    f.write(pruned_tflite_model_unstructured)\n",
    "\n",
    "print('Saved pruned TFLite model to:', os.path.abspath(pruned_tflite_file_unstructured))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'unstr_prunedweather'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(pruned_tflite_model_unstructured, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the unstructured pruned model: ', get_gzipped_model_size(pruned_tflite_file_unstructured))\n",
    "print('Size of the structured pruned model: ', get_gzipped_model_size(pruned_tflite_file))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('models/model_f32.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('models/model_f32.tflite') / get_gzipped_model_size(pruned_tflite_file_unstructured)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_tflite_file_unstructured = \"models/pruned_model_unstructured.tflite\"\n",
    "pruned_tflite_model_type_unstructured = \"Pruned model unstructured\"\n",
    "evaluate_model(pruned_tflite_file_unstructured, pruned_tflite_model_type_unstructured)\n",
    "print(\"Full-precision model accuracy is %.4f%% (Number of test samples=%d)\" % (fp_test_acc * 100, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_tflite_file_unstructured = \"models/pruned_model_unstructured.tflite\"\n",
    "pruned_tflite_model_type_unstructured = \"Pruned model unstructured\"\n",
    "pruned_tflite_pred_unstructured = run_tflite_model(pruned_tflite_file_unstructured, range(test_images.shape[0]))\n",
    "cm = confusion_matrix(test_labels, pruned_tflite_pred_unstructured);\n",
    "plot_confusion_matrix(cm, classes, title='Confusion matrix, without normalization of the pruned model unstructured', cmap=plt.cm.Spectral.reversed())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy of the model\n",
    "pruned_accuracy_unstructured = accuracy_score(test_labels, pruned_tflite_pred_unstructured)\n",
    "print('Unstructured pruned model accuracy is %.4f%% (Number of test samples=%d)' % (pruned_accuracy_unstructured * 100, len(test_images)))\n",
    "print('Structured pruned model accuracy is %.4f%% (Number of test samples=%d)' % (pruned_accuracy * 100, len(test_images)))\n",
    "print('Full-precision model accuracy is %.4f%% (Number of test samples=%d)' % (fp_test_acc * 100, len(test_images)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Unstructured pruning with dynamic sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstructured pruning with dynamic sparsity\n",
    "pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                                    final_sparsity=0.80,\n",
    "                                                                    begin_step=2000,\n",
    "                                                                    end_step=4000,\n",
    "                                                                    frequency=100)\n",
    "\n",
    "}\n",
    "\n",
    "# Create a pruning model\n",
    "pruned_model_unstructured_dynamic = tfmot.sparsity.keras.prune_low_magnitude(fp_model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "pruned_model_unstructured_dynamic.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_model_unstructured_dynamic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the pruned model\n",
    "pruned_model_unstructured_dynamic.fit(\n",
    "                    train_images,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_loss_unstructured_dynamic, pruned_acc_unstructured_dynamic = pruned_model_unstructured_dynamic.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Dynamically unstructured pruned model loss: ', pruned_loss_unstructured_dynamic)\n",
    "print('Dynamically unstructured pruned model accuracy: ', pruned_acc_unstructured_dynamic)\n",
    "print('Full-precision model accuracy: ', fp_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to Keras Lite\n",
    "pruned_model_unstructured_dynamic_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model_unstructured_dynamic)\n",
    "\n",
    "pruned_keras_file_unstructured_dynamic = 'models/pruned_model_unstructured_dynamic.h5'\n",
    "tf.keras.models.save_model(pruned_model_unstructured_dynamic_for_export, pruned_keras_file_unstructured_dynamic, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', os.path.abspath(pruned_keras_file_unstructured_dynamic))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model_unstructured_dynamic_for_export)\n",
    "pruned_tflite_model_unstructured_dynamic = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "pruned_tflite_file_unstructured_dynamic = 'models/pruned_model_unstructured_dynamic.tflite'\n",
    "\n",
    "with open(pruned_tflite_file_unstructured_dynamic, 'wb') as f:\n",
    "    f.write(pruned_tflite_model_unstructured_dynamic)\n",
    "\n",
    "print('Saved pruned TFLite model to:', os.path.abspath(pruned_tflite_file_unstructured_dynamic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write TFLite model to a C source (or header) file\n",
    "c_model_name = 'unstr_dyn_prunedweather'\n",
    "\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(pruned_tflite_model_unstructured_dynamic, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the unstructured dynamically pruned model: ', get_gzipped_model_size(pruned_tflite_file_unstructured_dynamic))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('models/model_f32.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('models/model_f32.tflite') / get_gzipped_model_size(pruned_tflite_file_unstructured_dynamic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_tflite_file_unstructured_dynamic = \"models/pruned_model_unstructured_dynamic.tflite\"\n",
    "pruned_tflite_model_type_unstructured_dynamic = \"Pruned model unstructured dynamic\"\n",
    "evaluate_model(pruned_tflite_file_unstructured_dynamic, pruned_tflite_model_type_unstructured_dynamic)\n",
    "print(\"Full-precision model accuracy is %.4f%% (Number of test samples=%d)\" % (fp_test_acc * 100, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_tflite_file_unstructured_dynamic = \"models/pruned_model_unstructured_dynamic.tflite\"\n",
    "pruned_tflite_model_type_unstructured_dynamic = \"Pruned model unstructured dynamic\"\n",
    "pruned_tflite_pred_unstructured_dynamic = run_tflite_model(pruned_tflite_file_unstructured_dynamic, range(test_images.shape[0]))\n",
    "cm = confusion_matrix(test_labels, pruned_tflite_pred_unstructured_dynamic);\n",
    "plot_confusion_matrix(cm, classes, title='Confusion matrix, without normalization of the pruned model unstructured dynamic', cmap=plt.cm.Spectral.reversed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy of the model\n",
    "pruned_accuracy_unstructured_dynamic = accuracy_score(test_labels, pruned_tflite_pred_unstructured_dynamic)\n",
    "print('Pruned model accuracy is %.4f%% (Number of test samples=%d)' % (pruned_accuracy_unstructured_dynamic * 100, len(test_images)))\n",
    "print('Full-precision model accuracy is %.4f%% (Number of test samples=%d)' % (fp_test_acc * 100, len(test_images)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization-aware training with pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PQAT\n",
    "quant_aware_annotate_model = tfmot.quantization.keras.quantize_annotate_model(\n",
    "              pruned_model_unstructured_for_export)\n",
    "\n",
    "pruned_qat_model = tfmot.quantization.keras.quantize_apply(quant_aware_annotate_model,\n",
    "                   tfmot.experimental.combine.Default8BitPrunePreserveQuantizeScheme())\n",
    "\n",
    "pruned_qat_model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "pruned_qat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "pruned_qat_model.fit(\n",
    "                    train_images,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_qat_loss, pruned_qat_acc = pruned_qat_model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Pruned QAT model loss: ', pruned_qat_loss)\n",
    "print('Pruned QAT model accuracy: ', pruned_qat_acc)\n",
    "print('Full-precision model accuracy: ', fp_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(pruned_qat_model)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# pruned_qat_tflite_model = converter.convert()\n",
    "\n",
    "# # Save the model\n",
    "# pruned_qat_tflite_file = 'models/pruned_qat_model.tflite'\n",
    "\n",
    "# with open(pruned_qat_tflite_file, 'wb') as f:\n",
    "#     f.write(pruned_qat_tflite_model)\n",
    "\n",
    "print('Saved pruned QAT TFLite model to:', os.path.abspath(pruned_qat_tflite_file))\n",
    "\n",
    "# write TFLite model to a C source (or header) file\n",
    "c_model_name = 'pruned_qat_weather'\n",
    "\n",
    "# with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "#     file.write(hex_to_c_array(pruned_qat_tflite_model, c_model_name))\n",
    "\n",
    "# compare the size of the pruned model and the full-precision model\n",
    "print('Size of the pruned QAT model: ', get_gzipped_model_size(pruned_qat_tflite_file))\n",
    "print('Size of th QAT model: ', get_gzipped_model_size( 'models/qat_int8.tflite'))\n",
    "print('Size of the full-precision model: ', get_gzipped_model_size('models/model_f32.tflite'))\n",
    "print(\"The achieved compression ratio is %.2fx\" % (get_gzipped_model_size('models/model_f32.tflite') / get_gzipped_model_size(pruned_qat_tflite_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_qat_tflite_file = \"models/pruned_qat_model.tflite\"\n",
    "pruned_qat_tflite_model_type = \"Pruned QAT model\"\n",
    "evaluate_model(pruned_qat_tflite_file, pruned_qat_tflite_model_type)\n",
    "print(\"Full-precision model accuracy is %.4f%% (Number of test samples=%d)\" % (fp_test_acc * 100, len(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "pruned_qat_tflite_file = \"models/pruned_qat_model.tflite\"\n",
    "pruned_qat_tflite_model_type = \"Pruned QAT model\"\n",
    "pruned_qat_tflite_pred = run_tflite_model(pruned_qat_tflite_file, range(test_images.shape[0]))\n",
    "cm = confusion_matrix(test_labels, pruned_qat_tflite_pred);\n",
    "plot_confusion_matrix(cm, classes, title='Confusion matrix, without normalization of the pruned QAT model', cmap=plt.cm.Spectral.reversed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy of the model\n",
    "pruned_qat_accuracy = accuracy_score(test_labels, pruned_qat_tflite_pred)\n",
    "print('Pruned QAT model accuracy is %.4f%% (Number of test samples=%d)' % (pruned_qat_accuracy * 100, len(test_images)))\n",
    "print('Full-precision model accuracy is %.4f%% (Number of test samples=%d)' % (fp_test_acc * 100, len(test_images)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "# create a table with the results\n",
    "model_types = ['full precision', 'PTQ', 'sPruned constant', 'uPruned constant', 'uPruned dynamic']\n",
    "#model_accuracies = [fp_test_acc, full_int8_accuracy, pruned_accuracy, pruned_accuracy_unstructured, pruned_accuracy_unstructured_dynamic, pruned_qat_accuracy]\n",
    "model_accuracies = [fp_test_acc, full_int8_accuracy, pruned_accuracy, pruned_accuracy_unstructured, pruned_accuracy_unstructured_dynamic]\n",
    "\n",
    "# cut precision to 4 decimal places\n",
    "model_accuracies = [round(x, 4) for x in model_accuracies]\n",
    "#model_sizes = [get_gzipped_model_size('models/model_f32.tflite'), get_gzipped_model_size('models/full_quant.tflite'), get_gzipped_model_size('models/pruned_model.tflite'), get_gzipped_model_size('models/pruned_model_unstructured.tflite'), get_gzipped_model_size('models/pruned_model_unstructured_dynamic.tflite'), get_gzipped_model_size('models/pruned_qat_model.tflite')]\n",
    "model_sizes = [get_gzipped_model_size('models/model_f32.tflite'), get_gzipped_model_size('models/full_quant.tflite'), get_gzipped_model_size('models/pruned_model.tflite'), get_gzipped_model_size('models/pruned_model_unstructured.tflite'), get_gzipped_model_size('models/pruned_model_unstructured_dynamic.tflite')]\n",
    "\n",
    "# divide by 1000 to get the size in KB\n",
    "model_sizes = [round(x / 1000, 2) for x in model_sizes]\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Model type\", \"Accuracy\", \"Size (KB)\"]\n",
    "for i in range(len(model_types)):\n",
    "    table.add_row([model_types[i], model_accuracies[i], model_sizes[i]])\n",
    "print(table)\n",
    "\n",
    "# plot model on x axis and accuracy on y axis\n",
    "# and make size of the points proportional to the size of the model\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.scatterplot(x=model_types, y=model_accuracies, size=model_sizes, sizes=(100, 1000), hue=model_types)\n",
    "# do not show the legend\n",
    "plt.legend([], [], frameon=False)\n",
    "# get only legend for model type\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "# only keep first 6 elements of dict\n",
    "by_label = {k: by_label[k] for k in list(by_label)[:6]}\n",
    "plt.legend(by_label.values(), by_label.keys(), title='Model type', loc='lower right')\n",
    "# annotate the points with the size of the model\n",
    "for i in range(len(model_types)):\n",
    "    plt.annotate(str(model_sizes[i]) + 'kB', (model_types[i], model_accuracies[i] + 0.0001))\n",
    "\n",
    "\n",
    "plt.title('Accuracy vs model type')\n",
    "plt.xlabel('Model type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
